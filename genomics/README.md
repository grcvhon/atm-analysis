# Integrating genomics in species distribution modelling
<br>
This directory contains scripts for genomic analyses as part of species distribution modelling.<br>
<br>
The idea here is to develop ancestry probability maps or calculate genetic distances which will be used to generate a raster layer that will inform the species distribution model.<br>
<br>
This README file is quite detailed as I treat it as a logbook to keep track of what I am doing in the command line.<br>
<br>
<i>NB: Data used are stored in the Phoenix HPC (University of Adelaide).</i>

<br>
<br>

## Outline

- [Prepare sample sheet of <i>Aipysurus apraefrontalis</i> and <i>A. foliosquama</i> individuals with RADseq data](#prepare-sample-sheet-of-aipysurus-apraefrontalis-and-a-foliosquama-individuals-with-radseq-data)
- [On kraken2 and UniVec databases](#on-kraken2-and-univec-databases)
- [Running ipyrad](#running-ipyrad)
- [Preparing `ipyrad` raw VCF file output](#preparing-ipyrad-raw-vcf-file-output)
  - [Filtering for high quality variants](#filtering-for-high-quality-variants)
  - [Examining filtered VCF](#examining-filtered-vcf)
  - [Principal Components Analysis](#principal-components-analysis)
- [`algatr`: A Landscape Genomic Analysis Toolkit in R](#algatr-a-landscape-genomic-analysis-toolkit-in-r)

---

### Prepare sample sheet of <i>Aipysurus apraefrontalis</i> and <i>A. foliosquama</i> individuals with RADseq data

This step involves collating information regarding samples of <i>Aipysurus apraefrontalis</i> and <i>A. foliosquama</i> that have available RADseq data (i.e., stored in `PhoenixHPC:/uofaresstor/sanders_lab/`). These data were generated by DArT (https://diversityarrays.com) and so the frequent use of DArT/DArTseq in this document.<br>

The goal of this collation is to generate a sample sheet that has at least the following information: `order`,`dart_id`,`id_clean` (for an example, see https://github.com/a-lud/sea-snake-dart/blob/main/data/sample-sheets/240524-sample-linkage.csv).<br>

* `order` corresponds to the DArT order number (`DNote##-####`)
* `dart_id` corresponds to the `.FASTQ.gz` prefix
* `id_clean` for example, <i>Hydrophis major</i> with KLS 1010 and FASTQ prefix 1234567: `HMA-KLS1010-1234567` (no whitespaces)
* `barcode9l`
* `barcode`

This format will improve efficiency when processing samples prior to any analyses and when using the workflow in genomics analyses.<br>

<i>NB: Scripts were not used entirely to collate information and some manual manipulation is required (e.g., via MS Excel).</i><br>

#### 1) Extract information from DArTseq master spreadsheet
First, we refer to the file: `DARTseq_master.xlsx` (version as of 11 February 2025). We then filter, in MS Excel, for <i>A. apraefrontalis</i> and <i>A. foliosquama</i>. Take note of some of the comments as some samples may have been contaminated or of just low quality. Nonetheless, we take all rows that are either <i>A. apraefrontalis</i> or <i>A. foliosquama</i>.<br>

We also added columns to contain information on latitude and longitude (if present, obtained from `The_One_Spreadsheet` and other field data sheets; files not stored in this repo), and if sample is usable (yes/no) based on information from `DARTseq_master.xlsx`.<br>

This spreadsheet was assembled manually and output is shown below (first 10 entries):

|Source                  |SampleID      |Genus    |Species       |Location    |Latitude    |Longitude  |DaRT_set    |FASTQ.gz|Comments                             |Use|
|------------------------|--------------|---------|--------------|------------|------------|-----------|------------|--------|-------------------------------------|---|
|PreATM_sampling         |Aaprae 4.12.01|Aipysurus|apraefrontalis|Ashmore Reef|-12.24174549|123.04166  |DNote21-6332|2562202 |Coordinates approximate              |yes|
|PreATM_sampling         |KLS0834       |Aipysurus|apraefrontalis|Exmouth Gulf|-22.166666  |114.2999988|DNote21-6332|2562130 |Coordinates approximate              |yes|
|PreATM_sampling         |SAM R68142    |Aipysurus|apraefrontalis|            |            |           |DNote21-6332|2571051 |Low quality DaRT                     |no |
|PreATM_sampling         |SS171013-03   |Aipysurus|apraefrontalis|Pilbara     |-19.6889305 |118.220874 |DNote21-6332|2562139 |                                     |yes|
|PreATM_sampling         |Afo1          |Aipysurus|foliosquama   |Ashmore Reef|-12.24174549|123.04166  |DNote21-6332|2562140 |Coordinates approximate              |yes|
|PreATM_sampling         |Afo8          |Aipysurus|foliosquama   |Ashmore Reef|-12.24174549|123.04166  |DNote21-6332|2562249 |Coordinates approximate              |yes|
|PreATM_sampling         |Afo8          |Aipysurus|foliosquama   |Ashmore Reef|-12.24174549|123.04166  |DNote21-6332|2571080 |Coordinates approximate              |yes|
|PreATM_sampling         |KLS1001       |Aipysurus|foliosquama   |            |            |           |DNote21-6332|2562209 |WA Coast apraefrontalis_contamination|no |
|PreATM_sampling         |KLS1001       |Aipysurus|foliosquama   |            |            |           |DNote21-6332|2584016 |WA Coast apraefrontalis_contamination|no |

<i>NB: For complete output, see: </i>`atm_genetic_dataset.csv`.
<br>

#### 2) Use command line to initialise our sample sheet file
From our `atm_genetic_dataset.csv` file, we want to initialise the first 3 columns of our sample sheet in the desired format. Using the following command, let us extract the samples that have a "yes" (i.e., usable) in the `Use` column of our `atm_genetic_dataset.csv`.
<br>
```bash
awk -F, '{ if ( $11 ~ /yes/ ) { print $2, $3, $4, $9, $11 } }' atm_genetic_dataset.csv
```
This command goes: if column 11 (`Use`) is "yes", print out information for these columns: `SampleID`, `Genus`, `Species`, `FASTQ.gz`, `Use`<br>
```
# output
Aaprae 4.12.01 Aipysurus apraefrontalis 2562202 yes
KLS0834 Aipysurus apraefrontalis 2562130 yes
SS171013-03 Aipysurus apraefrontalis 2562139 yes
Afo1 Aipysurus foliosquama 2562140 yes
Afo8 Aipysurus foliosquama 2562249 yes
Afo8 Aipysurus foliosquama 2571080 yes
SS171014-02 Aipysurus foliosquama 2562167 yes
KLS1484 Aipysurus apraefrontalis 3517861 yes
KLS1486 Aipysurus apraefrontalis 3517868 yes
KLS1490 Aipysurus apraefrontalis 3517879 yes
KLS1435 Aipysurus apraefrontalis 3593375 yes
KLS1436 Aipysurus apraefrontalis 3593362 yes
KLS1454 Aipysurus apraefrontalis 3593372 yes
KLS1457 Aipysurus apraefrontalis 3593394 yes
KLS1459 Aipysurus apraefrontalis 3593395 yes
KLS1465 Aipysurus apraefrontalis 3593393 yes
KLS1468 Aipysurus apraefrontalis 3593397 yes
KLS1477 Aipysurus apraefrontalis 3593356 yes
KLS1509 Aipysurus apraefrontalis 3593337 yes
KLS1202 Aipysurus foliosquama 3593377 yes
KLS1696 Aipysurus foliosquama 4013436 yes
KLS1700 Aipysurus foliosquama 4013440 yes
KLS1701 Aipysurus foliosquama 4013441 yes
KLS1702 Aipysurus foliosquama 4013442 yes
KLS1707 Aipysurus foliosquama 4013447 yes
KLS1708 Aipysurus foliosquama 4013448 yes
KLS1710 Aipysurus foliosquama 4013450 yes
```
Knowing that the command takes the samples we want, we can expand the command to produce the first 3 columns of our sample sheet file in the desired format.

```bash
# generate headers
echo "order","dart_id","id_clean","Genus","Species","Longitude","Latitude","Locality" > individuals.csv

# append output of command below on to the `individuals.csv` file
awk -F, '{ if ( $11 ~ /yes/ ) { gsub(/ /,"_");\
print $8"," $9","toupper(substr($3,1,1))toupper(substr($4,1,2))"-"$2"-"$9","\
$3","$4","$7","$6","$5 } }' atm_genetic_dataset.csv >> individuals.csv
```

Preview our [`individuals.csv`](https://github.com/grcvhon/atm-analysis/blob/master/genomics/sample-sheets/individuals.csv)<br>

We will add the `barcode9l` and `barcode` columns in the next steps.<br>
<br>

#### 3) Extract `barcode9l`,`barcode` information from DArTseq targets file
Each DArTseq order comes with a `targets_*.csv` file. This file has `barcode9l` and `barcode` columns which are additional bases appended specifically to each sample during the RAD sequencing protocol. We need to determine these barcodes to get raw sequences into usable form.

From our `individuals.csv` so far, we know the DArTseq order information. Taking the last for digits we have: `6332`,`8556`,`8773`, and `9763`. We will look into the targets file of these orders which are stored in `PhoenixHPC:/uofaresstor/sanders_lab/sequencing-datasets/radseq/`.<br>

To avoid any accidental/unwanted modifications to original files in the HPC server line (which are irreversible), let us download the `target_*.csv` files specific to each order from the Phoenix HPC into our local machine.<br>

```bash
rsync -at a1235304@p2-log-1.hpc.adelaide.edu.au:/uofaresstor/sanders_lab/sequencing-datasets/radseq/DaRT-DNote21-6332/targets_HLCFMDRXY_1.csv ./6332/
rsync -at a1235304@p2-log-1.hpc.adelaide.edu.au:/uofaresstor/sanders_lab/sequencing-datasets/radseq/DaRT-DNote23-8556/targets_HGHTWDRX3_1.csv ./8556/
rsync -at a1235304@p2-log-1.hpc.adelaide.edu.au:/uofaresstor/sanders_lab/sequencing-datasets/radseq/DaRT-DNote23-8773/targets_22FFK7LT3_2.csv ./8773/
rsync -at a1235304@p2-log-1.hpc.adelaide.edu.au:/uofaresstor/sanders_lab/sequencing-datasets/radseq/DaRT-DNote24-9763/targets_22* ./9763/
```

The commands above copied the `targets_*.csv` file from the relevant `DaRT-DNote##-##` directory into individual directories in our local machine corresponding to the last 4 digits of the order ID. (`6332`,`8556`,`8773`,`9763`).<br>

Now we can use our existing `individuals.csv` and the `targets_*.csv` files we downloaded to get the relevant `barcode9l` and `barcode` information.<br>

We use the following command to do so:
```bash
# generate column headers for `barcodes.csv`
echo targetid,barcode9l,barcode > barcodes.csv

# main for-loop
for i in $(awk -F, '{print $2}' individuals.csv | tail -n +2); 
    do awk -F, '$1==dart_id {print $1","$15","$16}' dart_id="$i" ./*/*.csv; 
        done >> barcodes.csv
```
The first part of the command just above lists the values in `dart_id` column of our `individuals.csv` file.<br>

The next part uses this list by going through each value and finding a match in the first column of the downloaded `targets_*.csv` files. It searches through each of the `targets_*.csv` files in each of the order directories and, if it finds a match, prints the `targetid` (i.e., `dart_id`), the `barcode9l`, and `barcode` columns (columns 15 and 16 in `targets_*.csv`).<br>

The output is then written as [`barcodes.csv`](https://github.com/grcvhon/atm-analysis/blob/master/genomics/sample-sheets/barcodes.csv)<br>

I included the `targetid` so I can compare it with the `dart_id` from the `individuals.csv`. This way we can make sure that the information for `barcode9l` and `barcode` are all in line with the same sample.<br>

To check, this command was used: `paste -d, individuals.csv barcodes.csv | awk '{ if ($2 == $4) print "yes" }'` which prints "yes" if `dart_id` ($2) in `individuals.csv` matches with `targetid` in `barcodes.csv` ($4).<br>
<br>

#### 4) Finalise the `sample-sheet.csv` file in the desired format
The goal of this exercise is to generate a sample sheet that follows this format: https://github.com/a-lud/sea-snake-dart/blob/main/data/sample-sheets/240524-sample-linkage.csv<br>

We will now put together the two `.csv` files we have generated: `individuals.csv` and `barcodes.csv`; and finalise our sample sheet.<br>

```bash
paste -d, individuals.csv barcodes.csv |\
awk -F, '{ print $1","$2","$3","$10","$11","$4","$5","$6","$7","$8 }' > sample-sheet.csv
```

Preview [`sample-sheet.csv`](https://github.com/grcvhon/atm-analysis/blob/master/genomics/sample-sheets/sample-sheet.csv). We can use now use this file as input in the genomics workflow (see https://github.com/a-lud/sea-snake-dart/tree/main) or begin with `01_rename_batch.sh` in this repository.<br>

[Back to top](#outline)

---

### On `kraken2` and UniVec databases

Before running `02_qc.sh`, make sure to download:
* latest `kraken2` standard database, and
* UniVec fasta file

#### 1) Download the latest `kraken2` standard database
Download the database from: https://benlangmead.github.io/aws-indexes/k2<br>
<br>
Under Collection > Standard, copy URL link of `tar.gz` file and run the following:
```bash
wget https://genome-idx.s3.amazonaws.com/kraken/k2_standard_20241228.tar.gz
tar -zxvf k2_standard_20241228.tar.gz -C k2_standard_20241228 # make sure the value supplied for `-C` exists
# Command just above should output the files in the directory name supplied.
```
Check for file integrity:
```bash
wget https://genome-idx.s3.amazonaws.com/kraken/standard_20241228/standard.md5
md5sum -c standard.md5 # run where the extracted files are located
```

#### 2) Download the UniVec fasta file
The UniVec fasta file can be downloaded from the NCBI database:
```bash
wget -O univec.fasta https://ftp.ncbi.nlm.nih.gov/pub/UniVec/UniVec
```

Proceed with running `02_qc.sh`. However, be aware that in some instances the `kraken2` step in `02_qc.sh` will output a `fastq` file in the incorrect format resulting in an empty `fastq` file at the end of the script run. ~~This error is still being troubleshooted.~~ Edit: `pixi` package manager appears to be causing the issue though not resolved (but see what happens in next section).
<br>

[Back to top](#outline)

---

### Running `ipyrad`

Originally went with `pixi` package manager to run scripts. But issues with `ipyrad` steps (e.g. 3 to 7) were persistent so I had to uninstall `pixi` and run `ipyrad` via `Anaconda`. See Troubleshoot note below.<br>

#### <i>Troubleshooting note</i>:

>For some reason, running `ipyrad` steps 3 through 7 under `denovo` assembly method encounters an error. This issue does not occur when supplying a reference genome (`reference ## [5] assembly_method`). ALud suspects the issue is with `pixi`. So I uninstalled `pixi` and removed all packages installed with it using commands below:
>```
>rm ~/.pixi/bin/pixi # removes binaries
>rm -r ~/.pixi # removes installed packages
>```
>Then I commented out the lines associated with `pixi` and `Anaconda` in my `~/.bashrc` file. I also removed the existing `ipyrad` conda environment that failed to install just to reset.<br>
><br>
>I restarted Phoenix to apply changes and then ran the commands:
>```
>conda init
>conda create -n ipyrad
>conda activate ipyrad
>conda install ipyrad -c conda-forge -c bioconda
>```

With that out of the way, let us actually run `ipyrad`:
```
# Step 1: Run steps 1 and 2 on all data
ipyrad -p params-all_samples_stringent-s12.txt -s 12

# Step 2: Create two branches - reference and denovo
ipyrad -p params-all_samples_stringent-s12.txt -b AFO-denovo AFO-samples.txt
ipyrad -p params-all_samples_stringent-s12.txt -b AFO-reference AFO-samples.txt

# Step 3: Edit the two new params files 
> Edit denovo argument in params-AFO-denovo.txt <
> Edit reference argument in params-AFO-reference.txt <

# Step 4: Run the remaining steps for each branch
ipyrad -p params-AFO-denovo.txt -s 34567
ipyrad -p params-AFO-reference.txt -s 34567
```
Can then repeat from step 2 for <i>A. apraefrontalis</i> (e.g. `...-b AAP-denovo AAP-samples.txt` and so on).<br>
<br>

After running these commands, the VCF files and other output will be stored in one of the following directories:
* `AFO-denovo_outfiles`
* `AFO-reference_outfiles`
* `AAP-denovo_outfiles`
* `AAP-reference_outfiles`

[Back to top](#outline)

---

### Preparing `ipyrad` raw VCF file output

#### <i>Filtering for high quality variants</i>
I filtered the raw VCF file output from `ipyrad` to only contain high quality SNPs. This step was done by running `05-vcf-filter-highQ.sh`. This script required species-specific `popmap` file and can be generated with the commands below:
```
awk -F, '{print $3,$10}' ../sample-sheet.csv | grep "AFO" > AFO-popmap.tsv
awk -F, '{print $3,$10}' ../sample-sheet.csv | grep "AAP" > AAP-popmap.tsv
```

We can now run `05-vcf-filter-highQ.sh` which will filter the VCF output from `ipyrad` (= raw/unfiltered) to only contain high quality SNPs (= filtered). This script is also the same as running `vcftools` with the filters set as follows: `--max-missing 0.9`, `--minQ 10`, `--minDP 3`
<br>

The `05-vcf-filter-highQ.sh` script will generate `AFO-reference.highQ.filtered.vcf.gz` and `AAP-reference.highQ.filtered.vcf.gz`.

[Back to top](#outline)

#### <i>Examining filtered VCF</i>
Examining the resulting VCF file, I particularly looked at the frequency of missing data per sample (shown in the following tables).<br>

Individual missingness for `AFO-reference.highQ.filtered`:                      
|INDV                   |N_DATA  |N_GENOTYPES_FILTERED      |N_MISS|F_MISS      |
|-----------------------|---------|-------------------------|------|------------|
|AFO-Afo1-2562140       | 6249    |0                        |51    | 0.00816131 |
|AFO-Afo8-2562249       | 6249    |0                        |1565  | 0.25044    |
|AFO-Afo8-2571080       | 6249    |0                        |1831  | 0.293007   |
|AFO-KLS1202-3593377    | 6249    |0                        |134   | 0.0214434  |
|AFO-KLS1696-4013436    | 6249    |0                        |86    | 0.0137622  |
|AFO-KLS1700-4013440    | 6249    |0                        |4     | 0.000640102|
|AFO-KLS1701-4013441    | 6249    |0                        |17    | 0.00272044 |
|AFO-KLS1702-4013442    | 6249    |0                        |16    | 0.00256041 |
|AFO-KLS1707-4013447    | 6249    |0                        |22    | 0.00352056 |
|AFO-KLS1708-4013448    | 6249    |0                        |9     | 0.00144023 |
|AFO-KLS1710-4013450    | 6249    |0                        |36    | 0.00576092 |
|AFO-SS171014-02-2562167| 6249    |0                        |27    | 0.00432069 |
<br>

Individual missingness for `AAP-reference.highQ.filtered`:    
|INDV    |N_DATA  |N_GENOTYPES_FILTERED    |N_MISS  |F_MISS|
|--------|--------|------------------------|--------|------|
|AAP-Aaprae_4.12.01-2562202      |5873    |0       |539     |0.0917759|
|AAP-KLS0834-2562130     |5873    |0       |209     |0.0355866|
|AAP-KLS1435-3593375     |5873    |0       |20      |0.00340541|
|AAP-KLS1436-3593362     |5873    |0       |9       |0.00153244|
|AAP-KLS1454-3593372     |5873    |0       |24      |0.0040865|
|AAP-KLS1457-3593394     |5873    |0       |45      |0.00766218|
|AAP-KLS1459-3593395     |5873    |0       |12      |0.00204325|
|AAP-KLS1465-3593393     |5873    |0       |8       |0.00136217|
|AAP-KLS1468-3593397     |5873    |0       |35      |0.00595948|
|AAP-KLS1477-3593356     |5873    |0       |28      |0.00476758|
|AAP-KLS1484-3517861     |5873    |0       |219     |0.0372893|
|AAP-KLS1486-3517868     |5873    |0       |405     |0.0689596|
|AAP-KLS1490-3517879     |5873    |0       |342     |0.0582326|
|AAP-KLS1509-3593337     |5873    |0       |17      |0.0028946|
|AAP-SS171013-03-2562139 |5873    |0       |121     |0.0206028|

Moving forward, for <i>A. foliosquama</i>: I will keep any sample that has `F_MISS` (= frequency of missing data) of < 0.01. For <i>A. apraefrontalis</i>: I will retain all samples as otherwise, there will be no representation from Ashmore Reef (key site).<br>

Keep individuals of <i>A. foliosquama</i> by running command below:
```bash
vcftools --gzvcf AFO-reference.highQ.filtered.vcf.gz --keep AFO-keep.txt --recode --stdout | gzip -c > AFO-reference.highQ.filtered.keep.vcf.gz
```


[Back to top](#outline)

#### <i>Principal Components Analysis</i>

I generated necessary files for a PCA plot using `plink2` running the command below:
```bash
plink2 --vcf ./ipyrad/AFO-reference_outfiles/AFO-reference.highQ.filtered.vcf.gz \
--double-id --allow-extra-chr --set-missing-var-ids @:# --pca \
--out ./AFO-reference.highQ.filtered --bad-freqs
```
I also prepared a PCA plot script function specific for AFO and AAP (see [`pca_plot.R`](https://github.com/grcvhon/atm-analysis/blob/master/genomics/scripts/pca_plot.R))

View the PCA plots here:
- [PCA plot for <i>A. foliosquama</i>](https://github.com/grcvhon/atm-analysis/blob/master/genomics/population-structure/plink_pca/AFO-reference.highQ.filtered.keep/AFO-reference.highQ.filtered.keep_pca.pdf)
- [PCA plot for <i>A. apraefrontalis</i>](https://github.com/grcvhon/atm-analysis/blob/master/genomics/population-structure/plink_pca/AAP-reference.highQ.filtered/AAP-reference.highQ.filtered_pca.pdf)

[Back to top](#outline)

---

### `algatr`: A Landscape Genomic Analysis Toolkit in R 

Paper: [Chambers et al. 2023. Individual-based landscape genomics for conservation: An analysis pipeline](https://onlinelibrary.wiley.com/doi/10.1111/1755-0998.13884)<br>

Originally, I thought of using the `algatr` package because it is "<i>a pipeline for performing cutting-edge landscape genomic analyses with individual-based sampling</i>" (Chambers et al. 2023). 
However, sample size and sample coverage across the area of interest proved to be significant factors which can limit the analyses performed through this toolkit.<br>

This issue is the case for <i>A. foliosquama</i> and <i>A. apraefrontalis</i>. As a workaround, I used <i>A. laevis</i> dataset given its greater sample size and better sample coverage across the area of interest. The <i>A. laevis</i> dataset I used was from [sea-snake-dart](https://github.com/a-lud/sea-snake-dart/tree/main), which has sampling across northern Australia (Shark Bay to Gulf of Carpentaria) and New Caledonia. I limited the dataset to only include samples from the northwest shelf.<br>

To begin, I used `vcftools` to only keep samples from the northwest shelf in the VCF file:
```bash
vcftools --vcf ALA-stringent.highQ.filtered.vcf --keep ../ALA-nw.txt --recode	--stdout
```
The file `ALA-nw.txt` was generated following scripts: [alaevis.R](https://github.com/grcvhon/atm-analysis/blob/master/genomics/scripts/alaevis.R) and [scratch.R](https://github.com/grcvhon/atm-analysis/blob/master/genomics/scripts/scratch.R). Refer to these scripts for the rest of workflow.<br>

I used the `TESS3` package in the `algatr` toolkit to generate a map with ancestry coefficients interpolated across the area of interest. See line 115 in `scratch.R`. I also wrote this into a *.tif file (`./genomics/algatr_TESS/laevis_nw_anc_coeff.tif`), which can be read into R:
```r
d <- raster::raster("./genomics/algatr_TESS/laevis_nw_anc_coeff.tif")
mapview::mapview(d)

```

[Back to top](#outline)

---

### Obtaining ocean surface current data

A couple of ways to obtain ocean surface current data. The original suggestion from VU is to use the `remora` package but issues with the `extractBlue()` function made me look for other alternatives. I ended up getting stuck (and into the rabbit hole) with OSCAR Ocean Surface Current data ([Global Ocean Surface Currents - Monthly Mean (2001-2020)](https://www.arcgis.com/home/item.html?id=b02f417ebbed4dc69edefd848dc69715) and [Ocean Surface Current Analyses Real-time (OSCAR) Surface Currents - Final 0.25 Degree (Version 2.0)](https://podaac.jpl.nasa.gov/dataset/OSCAR_L4_OC_FINAL_V2.0#)).<br>

The first OSCAR dataset can be downloaded as a `.pitemx` file, a file format which can be opened via ArcGIS. While the time slices (i.e., monthly mean values) can be exported and saved as a `.tif` separately. This operation has to be done manually as the python script needs the ArcGIS GUI to access information from the `.pitemx` file. Since tedious, I looked for another dataset which has `.nc` files; and so the latter dataset which has daily downloadable files spanning from 1993 to 2022 (~10,000 individual .nc files). This dataset provides very granular temporal resolution which might be too much for intended purpose. However, since I spent a lot of time figuring out how to download the dataset from [Earthdata](https://podaac.jpl.nasa.gov/), I share what I learned below:

>1) sign up for an Earthdata profile: https://urs.earthdata.nasa.gov/
>
>2) set up cmdline to recognise Earthdata credentials
>```bash
>cd ~
>touch .netrc
>echo "machine urs.earthdata.nasa.gov login toughturf password +0u6hturF9801" > .netrc
>chmod 0600 .netrc
>```
>
>3) create cookies for efficiency; makes credentials persist
>```bash
>cd ~
>touch .urs_cookies
>```
>
>4) download data - modify date range to download
>Ocean Surface Current Analyses Real-time (OSCAR) Surface Currents - Final 0.25 Degree (Version 2.0)
>url: https://podaac.jpl.nasa.gov/dataset/OSCAR_L4_OC_FINAL_V2.0#
>
>```bash
># install podaac-data-subscriber (https://podaac.github.io/tutorials/quarto_text/DataSubscriberDownloader.html)
>podaac-data-downloader -c OSCAR_L4_OC_FINAL_V2.0 -d ./ --start-date 1993-01-01T00:00:00Z --end-date 1993-01-08T00:00:00Z -e .nc
>```

<i>Last updated: 25 June 2025</i>
